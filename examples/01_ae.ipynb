{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and analyze autoencoders\n",
    "The first step of the BehaveNet pipeline is to compress the behavioral videos with a convolutional autoencoder (CAE), yielding a low-dimensional continuous representation of behavior that is useful for downstream analyses.\n",
    "\n",
    "Because the CAEs currently require significant computation time (generally >12 hours on a GPU) the data downloaded in the previous notebook also contains already trained CAEs, which we will analyze here.\n",
    "\n",
    "There are a variety of files that are automatically saved during the fitting of a CAE, which can be used for later analyses such as those below. Some of these files (many of which are common to all BehaveNet models, not just the CAE):\n",
    "* `best_val_model.pt`: the best CAE (not necessarily from the final training epoch) as determined by computing the loss on validation data\n",
    "* `meta_tags.csv`: hyperparameters associated with data, computational resources, and model\n",
    "* `metrics.csv`: metrics computed on dataset as a function of epochs; the default is that metrics are computed on training and validation data every epoch (and reported as a mean over all batches) while metrics are computed on test data only at the end of training using the best model (and reported per batch).\n",
    "* `[lab_id]_[expt_id]_[animal_id]_[session_id]_latents.pkl`: list of np.ndarrays of CAE latents computed using the best model\n",
    "* `session_info.csv`: sessions used to fit the model\n",
    "\n",
    "To fit your own CAEs, see additional documentation [here](https://behavenet.readthedocs.io/en/latest/source/user_guide.autoencoders.html). The downloaded CAEs used the default architecture with 9 latents, a learning rate of 1e-4, and no regularization. Models fit to individual datasets were trained for 600 epochs, while the model fit to both datasets was trained for 300 epochs.\n",
    "\n",
    "**Note**: The BehaveNet models are trained on batches of data, which here are defined as one trial per batch; at 189 frames per trial, 2 camera views, and 128x128 images, a batch of data is of size (189, 2, 128, 128). For datasets that do not have a trial structure (i.e. spontaneous behavior) we recommend splitting frames into arbitrarily defined \"trials\", the length of which should depend on the autocorrelation of the behavior (i.e. trials should not be shorter than the temporal extent of relevant behaviors). For the NP dataset in the original paper we used batch sizes of 1000 frames (~25 sec).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contents\n",
    "* [Plot train and val losses as a function of epochs](#Plot-train-and-val-losses-as-a-function-of-epochs)\n",
    "* [Plot train/val losses as a function of dataset](#Plot-train/val-losses-as-a-function-of-dataset)\n",
    "* [Make reconstruction movies](#Make-reconstruction-movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from behavenet import get_user_dir, make_dir_if_not_exists\n",
    "from behavenet.fitting.utils import get_expt_dir\n",
    "from behavenet.fitting.utils import get_session_dir\n",
    "from behavenet.fitting.utils import get_best_model_version\n",
    "from behavenet.fitting.utils import get_lab_example\n",
    "\n",
    "save_outputs = True  # true to save figures/movies to user's figure directory\n",
    "format = 'png'  # figure format ('png' | 'jpeg' | 'pdf'); movies saved as mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in train/validation/test: 382903/47863/47863\n"
     ]
    }
   ],
   "source": [
    "from behavenet.models import load_data as ld\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_dataset = ld.ParquetDataset(get_user_dir('data'), data_type = \"image\", split=\"train\")\n",
    "val_dataset = ld.ParquetDataset(get_user_dir('data'), data_type =\"image\", split=\"val\")\n",
    "test_dataset = ld.ParquetDataset(get_user_dir('data'), data_type=\"image\", split=\"test\")\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'Data in train/validation/test: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/jbw25/.conda/envs/behavenet/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /n/home13/jbw25/.conda/envs/behavenet/lib/python3.12 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-ce2b039a-7392-5c2c-aecb-fa9df6dee9ed]\n",
      "\n",
      "  | Name      | Type        | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | encoder   | ConvEncoder | 548 K  | train\n",
      "1 | decoder   | ConvDecoder | 549 K  | train\n",
      "2 | criterion | MSELoss     | 0      | train\n",
      "--------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.394     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52:  94%|█████████▍| 11307/11966 [02:27<00:08, 76.59it/s, v_num=58, val_loss=0.000825, train_loss=0.000821] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from behavenet.models import lightning_ae as ae\n",
    "\n",
    "\n",
    "# Initialize autoencoder with correct input shape\n",
    "autoencoder = ae.LightningAutoencoder(\n",
    "    input_channels=1,\n",
    "    input_height=140,\n",
    "    input_width=170,\n",
    "    latent_dim=9, \n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,  # Adjust based on convergence\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.fit(autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Save model\n",
    "#model_save_path = os.path.join(get_user_dir('models'), 'ae')\n",
    "#make_dir_if_not_exists(model_save_path)\n",
    "#autoencoder.save_checkpoint(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "train_losses = autoencoder.train_losses\n",
    "val_losses = autoencoder.val_losses\n",
    "val_losses = val_losses[:-1]\n",
    "\n",
    "# Plot across epochs\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation\", marker=\"s\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "test_batch = next(iter(test_loader))\n",
    "test_images = test_batch[0].to(autoencoder.device)\n",
    "\n",
    "# Forward pass through the autoencoder\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = autoencoder(test_images)\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "test_images_np = test_images.cpu().numpy()\n",
    "reconstructed_images_np = reconstructed_images.cpu().numpy()\n",
    "\n",
    "# Plot original and reconstructed images side by side\n",
    "n_images = 5  # Number of images to visualize\n",
    "fig, axes = plt.subplots(2, n_images, figsize=(15, 5))\n",
    "\n",
    "for i in range(n_images):\n",
    "    axes[0, i].imshow(test_images_np[i, 0], cmap=\"gray\")  # Original\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(reconstructed_images_np[i, 0], cmap=\"gray\")  # Reconstructed\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_title(\"Original Images\")\n",
    "axes[1, 0].set_title(\"Reconstructed Images\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "mse_loss = F.mse_loss(reconstructed_images, test_images)\n",
    "print(f\"Mean Squared Error (MSE): {mse_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "# Compute SSIM for a batch\n",
    "ssim_values = []\n",
    "for i in range(test_images_np.shape[0]):\n",
    "    ssim_value = ssim(test_images_np[i, 0], reconstructed_images_np[i, 0], data_range=1.0)\n",
    "    ssim_values.append(ssim_value)\n",
    "\n",
    "print(f\"Average SSIM: {np.mean(ssim_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot train and val losses as a function of epochs\n",
    "Note: plots similar to these can be automatically saved upon completion of CAE training by setting the `export_train_plots` option to `True` in the training json file.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet.fitting.utils import read_session_info_from_csv\n",
    "\n",
    "# set model info\n",
    "hparams = {\n",
    "    'data_dir': get_user_dir('data'),\n",
    "    'save_dir': get_user_dir('save'),\n",
    "    'experiment_name': 'ae-example',\n",
    "    'model_class': 'ae',\n",
    "    'model_type': 'conv',\n",
    "    'n_ae_latents': 9,\n",
    "}\n",
    "\n",
    "# programmatically fill out other hparams options\n",
    "get_lab_example(hparams, 'musall', 'vistrained')\n",
    "hparams['session_dir'], sess_ids = get_session_dir(hparams)\n",
    "hparams['expt_dir'] = get_expt_dir(hparams)\n",
    "\n",
    "# find metrics csv file\n",
    "versions = get_best_model_version(hparams['expt_dir'])\n",
    "version_dir = os.path.join(hparams['expt_dir'], 'version_%i' % versions[0])\n",
    "metric_file = os.path.join(version_dir, 'metrics.csv')\n",
    "metrics = pd.read_csv(metric_file)\n",
    "\n",
    "# collect data from csv file\n",
    "sess_ids = read_session_info_from_csv(os.path.join(version_dir, 'session_info.csv'))\n",
    "sess_ids_strs = []\n",
    "for sess_id in sess_ids:\n",
    "    sess_ids_strs.append(str('%s/%s' % (sess_id['animal'], sess_id['session'])))\n",
    "metrics_df = []\n",
    "for i, row in metrics.iterrows():\n",
    "    dataset = 'all' if row['dataset'] == -1 else sess_ids_strs[row['dataset']]\n",
    "    metrics_df.append(pd.DataFrame({\n",
    "        'dataset': dataset,\n",
    "        'epoch': row['epoch'],\n",
    "        'loss': row['val_loss'],\n",
    "        'dtype': 'val',\n",
    "    }, index=[0]))\n",
    "    metrics_df.append(pd.DataFrame({\n",
    "        'dataset': dataset,\n",
    "        'epoch': row['epoch'],\n",
    "        'loss': row['tr_loss'],\n",
    "        'dtype': 'train',\n",
    "    }, index=[0]))\n",
    "metrics_df = pd.concat(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "data_queried = metrics_df[(metrics_df.epoch > 20) & ~pd.isna(metrics_df.loss)]\n",
    "splt = sns.relplot(x='epoch', y='loss', hue='dtype', kind='line', data=data_queried)\n",
    "splt.ax.set_xlabel('Epoch')\n",
    "splt.ax.set_yscale('log')\n",
    "splt.ax.set_ylabel('MSE per pixel')\n",
    "\n",
    "if save_outputs:\n",
    "    save_file = os.path.join(get_user_dir('fig'), 'ae', 'loss_vs_epoch')\n",
    "    make_dir_if_not_exists(save_file)\n",
    "    plt.savefig(save_file + '.' + format, dpi=300, format=format)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot train/val losses as a function of dataset\n",
    "In the previous section we plotted training and validation losses for a CAE trained on a single experimental session, which was defined by the call to `get_lab_example()`; this function uses the example dataset defined in the `.behavenet/musall_vistrained_params` json file (`05-Dec-2017`). To choose the other dataset, you can modify the above cell like so:\n",
    "```python\n",
    "...\n",
    "get_lab_example(hparams, 'musall', 'vistrained')\n",
    "hparams['session'] = '07-Dec-2017'  # <- add this line\n",
    "...\n",
    "```\n",
    "\n",
    "There is a third option, which is to plot performance of the model trained on both datasets simultaneously:\n",
    "```python\n",
    "...\n",
    "get_lab_example(hparams, 'musall', 'vistrained')\n",
    "hparams['session'] = 'multisession-00'\n",
    "...\n",
    "```\n",
    "\n",
    "The session name `multisession-00` is a bit cryptic - with many datasets, this would correspond to only one of many different combinations. The datasets which are associated with this multisession dataset can be found in the `session_info.csv` file inside the multisession directory, i.e. `save_dir/musall/vistrained/mSM36/multisession-00/session_info.csv`. \n",
    "\n",
    "Below we will plot CAE performance on both datasets throughout training. [Note that this CAE was trained using half the number of epochs as the previous one since there was twice as many batches when combining the datasets.]\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet.fitting.utils import read_session_info_from_csv\n",
    "\n",
    "# set model info\n",
    "hparams = {\n",
    "    'data_dir': get_user_dir('data'),\n",
    "    'save_dir': get_user_dir('save'),\n",
    "    'experiment_name': 'ae-example',\n",
    "    'model_class': 'ae',\n",
    "    'model_type': 'conv',\n",
    "    'n_ae_latents': 9,\n",
    "}\n",
    "\n",
    "# programmatically fill out other hparams options\n",
    "get_lab_example(hparams, 'musall', 'vistrained')\n",
    "hparams['session'] = 'multisession-00'\n",
    "hparams['session_dir'], sess_ids = get_session_dir(hparams)\n",
    "hparams['expt_dir'] = get_expt_dir(hparams)\n",
    "\n",
    "# find metrics csv file\n",
    "versions = get_best_model_version(hparams['expt_dir'])\n",
    "version_dir = os.path.join(hparams['expt_dir'], 'version_%i' % versions[0])\n",
    "metric_file = os.path.join(version_dir, 'metrics.csv')\n",
    "metrics = pd.read_csv(metric_file)\n",
    "\n",
    "# collect data from csv file\n",
    "sess_ids = read_session_info_from_csv(os.path.join(version_dir, 'session_info.csv'))\n",
    "sess_ids_strs = []\n",
    "for sess_id in sess_ids:\n",
    "    sess_ids_strs.append(str('%s/%s' % (sess_id['animal'], sess_id['session'])))\n",
    "metrics_df = []\n",
    "for i, row in metrics.iterrows():\n",
    "    dataset = 'Combined' if row['dataset'] == -1 else sess_ids_strs[row['dataset']]\n",
    "    metrics_df.append(pd.DataFrame({\n",
    "        'dataset': dataset,\n",
    "        'epoch': row['epoch'],\n",
    "        'loss': row['val_loss'],\n",
    "        'dtype': 'val',\n",
    "    }, index=[0]))\n",
    "    metrics_df.append(pd.DataFrame({\n",
    "        'dataset': dataset,\n",
    "        'epoch': row['epoch'],\n",
    "        'loss': row['tr_loss'],\n",
    "        'dtype': 'train',\n",
    "    }, index=[0]))\n",
    "metrics_df = pd.concat(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "dtype = 'val'  # 'train' | 'val'\n",
    "data_queried = metrics_df[\n",
    "    (metrics_df.epoch > 20) & (metrics_df.dtype == dtype) & ~pd.isna(metrics_df.loss)]\n",
    "splt = sns.relplot(x='epoch', y='loss', hue='dataset', kind='line', data=data_queried)\n",
    "splt.ax.set_xlabel('Epoch')\n",
    "splt.ax.set_yscale('log')\n",
    "splt.ax.set_ylabel('MSE per pixel')\n",
    "# plt.title('%s loss' % title_str)\n",
    "\n",
    "if save_outputs:\n",
    "    save_file = os.path.join(get_user_dir('fig'), 'ae', 'loss_vs_epoch_by_dataset')\n",
    "    make_dir_if_not_exists(save_file)\n",
    "    plt.savefig(save_file + '.' + format, dpi=300, format=format)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reconstruction movies\n",
    "The above plots are useful, for example, if you want to determine whether or not the model training completed satisfactorily. They are not useful, however, in understanding how good the reconstructions actually look. In order to do so BehaveNet has functionality to make reconstruction movies that contain the original video, the reconstructed video, and the residual.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet.plotting.ae_utils import make_ae_reconstruction_movie_wrapper\n",
    "from behavenet.data.utils import get_data_generator_inputs\n",
    "from behavenet.data.data_generator import ConcatSessionsGenerator\n",
    "\n",
    "# movie info\n",
    "save_outputs = True\n",
    "include_linear = False  # True to include reconstructions from linear models; need training\n",
    "\n",
    "# set model info\n",
    "version = 'best'  # test-tube version; 'best' finds the version with the lowest mse\n",
    "sess_idx = 0  # when using a multisession, this determines which session is used\n",
    "hparams = {\n",
    "    'data_dir': get_user_dir('data'),\n",
    "    'save_dir': get_user_dir('save'),\n",
    "    'experiment_name': 'ae-example',\n",
    "    'lin_experiment_name': 'ae-example',\n",
    "    'model_class': 'ae',\n",
    "    'model_type': 'conv',\n",
    "    'n_ae_latents': 9,\n",
    "    'frame_rate': 20,  # frame rate of rendered movie, not original behavioral video\n",
    "}\n",
    "\n",
    "# programmatically fill out other hparams options\n",
    "get_lab_example(hparams, 'musall', 'vistrained')   \n",
    "hparams['session_dir'], sess_ids = get_session_dir(hparams)\n",
    "hparams['expt_dir'] = get_expt_dir(hparams)\n",
    "\n",
    "# load data generator to find a test trial\n",
    "hparams, signals, transforms, paths = get_data_generator_inputs(hparams, sess_ids)\n",
    "data_generator = ConcatSessionsGenerator(\n",
    "    hparams['data_dir'], sess_ids, \n",
    "    signals_list=signals, transforms_list=transforms, paths_list=paths,\n",
    "    device='cpu', as_numpy=False, batch_load=True, rng_seed=0)\n",
    "print(data_generator)\n",
    "trial = data_generator.datasets[sess_idx].batch_idxs['test'][2]  # trial to use in movie\n",
    "\n",
    "filename = str('D=%02i_recon_ae' % hparams['n_ae_latents'])\n",
    "if include_linear:\n",
    "    filename += '_wlinear'\n",
    "\n",
    "make_ae_reconstruction_movie_wrapper(\n",
    "    hparams, version=version, \n",
    "    save_file=os.path.join(get_user_dir('fig'), 'ae', filename), \n",
    "    include_linear=include_linear, trial=trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
